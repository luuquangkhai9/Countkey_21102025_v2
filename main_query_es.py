from elasticsearch import Elasticsearch
import json
from datetime import datetime, timedelta
from dotenv import load_dotenv
import logging
import urllib3
import time
# es = Elasticsearch(["http://172.168.200.202:9200"] , request_timeout=100)
import os

import re
from tqdm import tqdm
import py_vncorenlp
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

# L·∫•y URL Elasticsearch t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
elasticsearch_url = os.getenv("ELASTICSEARCH_URL")
# Kh·ªüi t·∫°o Elasticsearch client
es = Elasticsearch([elasticsearch_url], request_timeout=100)
logger = logging.getLogger("run")  # S·ª≠ d·ª•ng logger ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
logging.getLogger("urllib3").propagate = False

# Gi·∫£m log cho Elasticsearch (n·∫øu c·∫ßn thi·∫øt)
logging.getLogger("elasticsearch").setLevel(logging.CRITICAL)
# Remove all handlers from urllib3
class ExcludeHttpLogsFilter(logging.Filter):
    def filter(self, record):
        # L·ªçc b·ªè c√°c log ch·ª©a "POST" ho·∫∑c "HEAD"
        return "POST" not in record.getMessage() and "HEAD" not in record.getMessage()

# Th√™m b·ªô l·ªçc v√†o logger
logging.getLogger("elasticsearch").addFilter(ExcludeHttpLogsFilter())
logging.getLogger("urllib3").addFilter(ExcludeHttpLogsFilter())

def query_keyword_with_topic(es, start_date_str, end_date_str, type):
    try:
        logger.info(f"Starting query_keyword_with_topic for type: {type}, from {start_date_str} to {end_date_str}")

        # H√†m chung ƒë·ªÉ truy v·∫•n Elasticsearch v·ªõi search_after
        def fetch_records(index, query_body):
            records = []
            search_after_value = None  # L∆∞u gi√° tr·ªã search_after

            try:
                while True:
                    # N·∫øu ƒë√£ c√≥ search_after_value, th√™m v√†o body
                    if search_after_value:
                        query_body["search_after"] = search_after_value

                    # Th·ª±c hi·ªán truy v·∫•n
                    result = es.options(request_timeout=2000).search(index=index, body=query_body)

                    hits = result['hits']['hits']
                    if not hits:
                        logger.info(f"No more records found in index: {index}")

                        break  # K·∫øt th√∫c khi kh√¥ng c√≤n d·ªØ li·ªáu

                    for hit in hits:
                        # X·ª≠ l√Ω d·ªØ li·ªáu cho t·ª´ng ch·ªâ m·ª•c
                        if index == 'posts':
                            if 'topic_id' not in hit['_source']:
                                hit['_source']['topic_id'] = []
                            # if 'hashtag' not in hit['_source']:
                            #     hit['_source']['hashtag'] = []

                            keywords = hit['_source'].get('keyword', [])
                            keywords = keywords if isinstance(keywords, list) else []

                            key_words = hit['_source'].get('key_word', [])
                            key_words = key_words if isinstance(key_words, list) else []

                            # L·∫•y danh s√°ch t·ª´ theo ƒëi·ªÅu ki·ªán t·ª´ tr∆∞·ªùng key_word_type
                            key_word_type = hit['_source'].get('key_word_type', [])
                            filtered_words = []

                            if isinstance(key_word_type, list) and key_word_type:
                                for item in key_word_type:
                                    if isinstance(item, list) and len(item) == 2:  # ƒê·∫£m b·∫£o item l√† danh s√°ch c√≥ 2 ph·∫ßn t·ª≠
                                        w, pos = item
                                        # if (pos in ['Np', 'Ny', 'Nb', 'Vb'] or (pos in ['N'] and '_' in w and not w.startswith('_'))):
                                        if (pos in ['Np', 'Ny', 'Nb', 'Vb'] or (pos in ['N'] and '_' in w and not w.startswith('_'))):

                                            filtered_words.append(w.lower())
                                # N·∫øu key_word_type kh√¥ng r·ªóng, ch·ªâ l·∫•y c√°c t·ª´ t·ª´ ƒë√¢y
                                combined_keywords = list(set(filtered_words))
                            else:
                                # N·∫øu key_word_type r·ªóng, k·∫øt h·ª£p keywords v√† key_word
                                combined_keywords = list(set(keywords + key_words))

                            # C·∫≠p nh·∫≠t l·∫°i keyword trong hit['_source']
                            hit['_source']['keyword'] = combined_keywords

                            # X√≥a c√°c tr∆∞·ªùng kh√¥ng c·∫ßn thi·∫øt
                            if 'key_word' in hit['_source']:
                                del hit['_source']['key_word']
                            if 'key_word_type' in hit['_source']:
                                del hit['_source']['key_word_type']

                            

                            hashtag = hit['_source'].get('hashtag', [])
                            hashtag = hashtag if isinstance(hashtag, list) else []

                            hit['_source']['hashtag'] = hashtag

                        elif index == 'comments':
                            if 'topic_id' not in hit['_source']:
                                hit['_source']['topic_id'] = []

                            keywords = hit['_source'].get('keywords', [])
                            keywords = keywords if isinstance(keywords, list) else []

                            key_words = hit['_source'].get('key_word', [])
                            key_words = key_words if isinstance(key_words, list) else []
                            
                            key_word_type = hit['_source'].get('key_word_type', [])
                            filtered_words = []

                            if isinstance(key_word_type, list) and key_word_type:
                                for item in key_word_type:
                                    if isinstance(item, list) and len(item) == 2:  # ƒê·∫£m b·∫£o item l√† danh s√°ch c√≥ 2 ph·∫ßn t·ª≠
                                        w, pos = item
                                        if (pos in ['Np', 'Ny', 'Nb', 'Vb'] or (pos in ['N'] and '_' in w and not w.startswith('_'))):
                                            filtered_words.append(w.lower())
                                # N·∫øu key_word_type kh√¥ng r·ªóng, ch·ªâ l·∫•y c√°c t·ª´ t·ª´ ƒë√¢y
                                combined_keywords = list(set(filtered_words))
                            else:
                                combined_keywords = list(set(keywords + key_words))
                            hit['_source']['keywords'] = combined_keywords

                            hashtags = hit['_source'].get('hashtags', [])
                            hashtags = hashtags if isinstance(hashtags, list) else []

                            hashtag = hit['_source'].get('hashtag', [])
                            hashtag = hashtag if isinstance(hashtag, list) else []

                            combined_hashtag = list(set(hashtags + hashtag))
                            hit['_source']['hashtags'] = combined_hashtag
                            if 'key_word' in hit['_source']:
                                del hit['_source']['key_word']
                            if 'key_word_type' in hit['_source']:
                                del hit['_source']['key_word_type']

                            if 'hashtag' in hit['_source']:
                                del hit['_source']['hashtag']

                        records.append(hit)

                    # L·∫•y gi√° tr·ªã sort cu·ªëi c√πng t·ª´ k·∫øt qu·∫£ ƒë·ªÉ s·ª≠ d·ª•ng cho search_after
                    search_after_value = hits[-1]['sort']
                    # logger.info(f"Fetched {len(hits)} records from index: {index}, continuing...")

            except Exception as e:
                logger.error(f"Error fetching records from {index}: {e}")

            return records  # ƒê·∫£m b·∫£o tr·∫£ v·ªÅ danh s√°ch (c√≥ th·ªÉ r·ªóng)

        # ƒê·ªãnh nghƒ©a truy v·∫•n Elasticsearch cho c·∫£ hai ch·ªâ m·ª•c
        body_posts = {
            "query": {
                "bool": {
                    "must": [
                        {"match_phrase": {"type": type}},
                        {"range": {"created_time": {"gte": start_date_str, "lte": end_date_str}}}
                    ]
                }
            },
            "sort": [{"created_time": {"order": "asc"}}, {"time_crawl.keyword": {"order": "asc"}}],
            "_source": ["keyword", "hashtag", "created_time", "topic_id", "key_word", "key_word_type","tenancy_ids"],
            "size": 4000
        }

        body_comments = {
            "query": {
                "bool": {
                    "must": [
                        {"match_phrase": {"type": type}},
                        {"range": {"created_time": {"gte": start_date_str, "lte": end_date_str}}}
                    ]
                }
            },
            "sort": [{"created_time": {"order": "asc"}}, {"time_crawl.keyword": {"order": "asc"}}],
            "_source": ["keywords", "hashtags", "created_time", "topic_id", "key_word", "hashtag", "key_word_type","tenancy_ids"],
            "size": 4000
        }

        # L·∫•y d·ªØ li·ªáu t·ª´ c·∫£ hai ch·ªâ m·ª•c
        records_posts = fetch_records('posts', body_posts)
        logger.info(f"Fetched {len(records_posts)} records from posts index.")

        records_comments = fetch_records('comments', body_comments)
        logger.info(f"Fetched {len(records_comments)} records from comments index.")

        # G·ªôp d·ªØ li·ªáu t·ª´ c·∫£ hai ch·ªâ m·ª•c
        records = records_posts + records_comments
        logger.info(f"Total records fetched: {len(records)}")

        # L∆∞u d·ªØ li·ªáu v√†o file JSON
        # with open(query_data_file, 'w', encoding='utf-8') as f:
        #     json.dump(records, f, ensure_ascii=False, indent=4)

        return records

    except Exception as e:
        logger.error(f"An error occurred during query_keyword_with_topic: {e}")

        return []
    

# def query_keyword_with_trend(es,INDEX, body):
#     try:


#         # H√†m chung ƒë·ªÉ truy v·∫•n Elasticsearch v·ªõi search_after
#         def fetch_records(index, query_body):
#             records = []
#             search_after_value = None  # L∆∞u gi√° tr·ªã search_after

#             try:
#                 while True:
#                     # N·∫øu ƒë√£ c√≥ search_after_value, th√™m v√†o body
#                     if search_after_value:
#                         query_body["search_after"] = search_after_value

#                     # Th·ª±c hi·ªán truy v·∫•n
#                     result = es.options(request_timeout=2000).search(index=index, body=query_body)

#                     hits = result['hits']['hits']
#                     if not hits:
#                         logger.info(f"No more records found in index: {index}")

#                         break  # K·∫øt th√∫c khi kh√¥ng c√≤n d·ªØ li·ªáu

#                     for hit in hits:
#                         # X·ª≠ l√Ω d·ªØ li·ªáu cho t·ª´ng ch·ªâ m·ª•c
#                         records.append(hit["_source"])

#                     # L·∫•y gi√° tr·ªã sort cu·ªëi c√πng t·ª´ k·∫øt qu·∫£ ƒë·ªÉ s·ª≠ d·ª•ng cho search_after
#                     search_after_value = hits[-1]["sort"]
#                     # logger.info(f"Fetched {len(hits)} records from index: {index}, continuing...")

#             except Exception as e:
#                 logger.error(f"Error fetching records from {index}: {e}")

#             return records  # ƒê·∫£m b·∫£o tr·∫£ v·ªÅ danh s√°ch (c√≥ th·ªÉ r·ªóng)

#         # ƒê·ªãnh nghƒ©a truy v·∫•n Elasticsearch cho c·∫£ hai ch·ªâ m·ª•c


#         # L·∫•y d·ªØ li·ªáu t·ª´ c·∫£ hai ch·ªâ m·ª•c


#         records = fetch_records(INDEX, body)
#         logger.info(f"Total records fetched: {len(records)}")

#         # L∆∞u d·ªØ li·ªáu v√†o file JSON
#         # with open(query_data_file, 'w', encoding='utf-8') as f:
#         #     json.dump(records, f, ensure_ascii=False, indent=4)

#         return records
#     except Exception as e:
#         logger.error(f"An error occurred during query_keyword_with_topic: {e}")

#         return []



VNCORE_MODEL_DIR = r'C:\Users\Administrator\Documents\Intern_Source\Countkey_21102025_v2\vncorenlp'
# BLACKLIST_FILE_PATH = r"C:\Users\Administrator\Documents\Fixed_key_countkey\blacklist_keywords.txt"

logger.info(f"ƒêang kh·ªüi t·∫°o VnCoreNLP t·ª´: {VNCORE_MODEL_DIR} (ch·ªâ m·ªôt l·∫ßn)...")
print(f"ƒêang kh·ªüi t·∫°o VnCoreNLP t·ª´: {VNCORE_MODEL_DIR} (ch·ªâ m·ªôt l·∫ßn)...")
try:
    VNCORE_MODEL = py_vncorenlp.VnCoreNLP(save_dir=VNCORE_MODEL_DIR)
    logger.info("‚úÖ VnCoreNLP ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng.")
    print("‚úÖ VnCoreNLP ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng.")
except Exception as e:
    logger.error(f"‚ùå KH√îNG TH·ªÇ KH·ªûI T·∫†O VNCORENLP. L·ªói: {e}")
    print(f"‚ùå KH√îNG TH·ªÇ KH·ªûI T·∫†O VNCORENLP. L·ªói: {e}")
    # exit()
    
# TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50)

# # --- 1.2: C√°c h√†m v√† quy t·∫Øc tr√≠ch xu·∫•t ---

# def load_stopwords(file_path):
#     """T·∫£i stopwords t·ª´ file."""
#     stopwords = []
#     try:
#         with open(file_path, "r", encoding="utf-8") as f:
#             for line in f:
#                 word = line.strip().strip("'")
#                 if word:
#                     stopwords.append(word.lower())
#         logger.info(f"‚úÖ ƒê√£ t·∫£i {len(stopwords)} t·ª´ kh√≥a blacklist t·ª´: {file_path}")
#         return tuple(stopwords)
#     except FileNotFoundError:
#         logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file blacklist: {file_path}. S·∫Ω ti·∫øp t·ª•c m√† kh√¥ng c√≥ blacklist.")
#         return tuple()

# BLACKLISTED_START_WORDS = load_stopwords(BLACKLIST_FILE_PATH)

# CHUNK_GRAMMAR = [
#     ("LEGAL_DOC_RULE_2", r"(?:<Np>|<N>)(?:\s+<N>)*(?:\s+<M>)+(?:\s+(?:<Np>|<N>|<Ny>))+"),
#     ("LEGAL_DOC_RULE", r"(?:<Np>|<N>)(?:\s+<N>)*(\s+<M>)+"),
#     ("NOUN_RULE", r"(?:<Np>|<N>|<Ny>)(?:\s+(?:<Np>|<N>|<Ny>))*"),
# ]


# def compile_rules(rules):
#     """Bi√™n d·ªãch c√°c quy t·∫Øc regex."""
#     return [
#         (name, re.compile(rule.replace("<", r"(?:\\b").replace(">", r"\\b)")))
#         for name, rule in rules
#     ]

# COMPILED_CHUNK_RULES = compile_rules(CHUNK_GRAMMAR)

# def clean_text(text):
#     """D·ªçn d·∫πp vƒÉn b·∫£n c∆° b·∫£n."""
#     if not text:
#         return ""
#     text = re.sub(r'[-‚Äì‚Äî]+', ' ', text)
#     text = ' '.join(text.split())
#     return text

# def is_valid_single_word_keyword(chunk_word, rule_name):
#     """Ki·ªÉm tra xem t·ª´ ƒë∆°n c√≥ h·ª£p l·ªá kh√¥ng."""
#     tag = chunk_word.get('posTag')
#     word_form = chunk_word.get('wordForm', '')
#     if tag == 'Np' and len(word_form) > 5:
#         return True
#     if tag == 'Ny':
#         if word_form.isupper() and len(word_form) >= 2:
#             return True
#     if rule_name == "VERB_RULE" and tag == 'V' and len(word_form) > 5:
#         return True
#     return False

# def extract_keywords_2(annotated_data):
#     """H√†m tr√≠ch xu·∫•t t·ª´ kh√≥a ch√≠nh t·ª´ d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c VnCoreNLP g√°n nh√£n."""
#     seen_keywords_lower = set()
#     ordered_keywords = []
#     if not isinstance(annotated_data, dict):
#         return []
#     for sentence_words in annotated_data.values():
#         if not sentence_words:
#             continue
#         pos_sequence = " ".join([word.get('posTag', '') for word in sentence_words])
#         claimed_indices = [False] * len(sentence_words)
#         for rule_name, rule_pattern in COMPILED_CHUNK_RULES:
#             for match in rule_pattern.finditer(pos_sequence):
#                 start_char, end_char = match.span()
#                 start_word = len(pos_sequence[:start_char].strip().split()) if start_char > 0 else 0
#                 end_word = len(pos_sequence[:end_char].strip().split())
#                 if any(claimed_indices[i] for i in range(start_word, end_word)):
#                     continue
#                 chunk_words = sentence_words[start_word:end_word]
#                 phrase_text = ' '.join(w.get('wordForm', '') for w in chunk_words).replace("_", " ")
#                 is_valid = False
#                 if len(chunk_words) > 1:
#                     is_valid = True
#                 elif len(chunk_words) == 1:
#                     if is_valid_single_word_keyword(chunk_words[0], rule_name):
#                         is_valid = True
#                 if is_valid and phrase_text:
#                     lower_phrase = phrase_text.lower()
#                     if lower_phrase not in seen_keywords_lower:
#                         is_blacklisted = False
#                         for blacklisted_word in BLACKLISTED_START_WORDS:
#                             if blacklisted_word in lower_phrase:
#                                 is_blacklisted = True
#                                 break
#                         if is_blacklisted:
#                             continue
#                         seen_keywords_lower.add(lower_phrase)
#                         ordered_keywords.append(phrase_text)
#                         for i in range(start_word, end_word):
#                             claimed_indices[i] = True
#     return ordered_keywords

# def get_keywords_for_document(title, content):
#     """
#     H√†m t·ªïng h·ª£p: Nh·∫≠n title v√† content, tr·∫£ v·ªÅ danh s√°ch keywords cu·ªëi c√πng.
#     """
#     combined_text = f"{title}. {content}" if title else content
#     if not combined_text.strip() or combined_text.strip() == '.':
#         return []

#     chunks = TEXT_SPLITTER.split_text(combined_text)

#     if len(chunks) > 3:
#         logger.warning(f"  B√†i vi·∫øt c√≥ {len(chunks)} chunks. Ch·ªâ x·ª≠ l√Ω chunk ƒë·∫ßu ti√™n.")
#         chunks = chunks[:1] 
    
#     final_keywords_for_article = []
#     seen_keywords_for_article = set()
    
#     for chunk in chunks:
#         cleaned_chunk = clean_text(chunk)
#         if not cleaned_chunk:
#             continue
        
#         annotated_chunk = VNCORE_MODEL.annotate_text(cleaned_chunk)
#         keywords_from_chunk = extract_keywords_2(annotated_chunk)
        
#         for keyword in keywords_from_chunk:
#             lower_keyword = keyword.lower()
#             if lower_keyword not in seen_keywords_for_article:
#                 seen_keywords_for_article.add(lower_keyword)
#                 final_keywords_for_article.append(keyword)
    
#     return final_keywords_for_article

def upgrade_extract_keyword_record(es, target_index_alias):
    # """
    # Truy v·∫•n 'posts' trong ng√†y, tr√≠ch xu·∫•t t·ª´ kh√≥a v√† ƒë·∫©y sang index m·ªõi.
    # S·ª≠ d·ª•ng global 'es' client.
    # """
    # index_root = "posts"
    # batch_size = 500
    # bulk_batch_size = 1000

    # time_current = datetime.now()
    # start_of_day = time_current.replace(hour=0, minute=0, second=0, microsecond=0)
    # end_of_day = time_current
    
    # time_format = "MM/dd/yyyy HH:mm:ss"
    
    # fields_to_extract = [
    #     "id", "created_time", "time_crawl", "type", "field_classify", 
    #     "link", "author", "topic_id", "tenancy_ids", "title", "content"
    # ]

    # query_body = {
    #     "size": batch_size,
    #     "_source": fields_to_extract,
    #     "query": {
    #         "bool": {
    #             "filter": [
    #                 {"range": {"created_time": {"gte": start_of_day.strftime(time_format), "lte": end_of_day.strftime(time_format), "format": time_format}}},
    #                 {"term": {"type.keyword": "electronic media"}}
    #             ]
    #         }
    #     },
    #     "sort": [{"created_time": "asc"}, {"_id": "asc"}]
    # }

    # search_after_value = None
    # records_to_bulk = []
    # total_processed = 0
    
    # logger.info(f"B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t t·ª´ index '{index_root}' (Lo·∫°i: 'electronic media')")
    # logger.info(f"Ph·∫°m vi th·ªùi gian: {start_of_day} T·ªöI {end_of_day}")

    # try:
    #     while True:
    #         if search_after_value:
    #             query_body["search_after"] = search_after_value

    #         result = es.search(index=index_root, body=query_body, request_timeout=120)
    #         hits = result['hits']['hits']
    #         if not hits:
    #             logger.info("Kh√¥ng c√≤n d·ªØ li·ªáu n√†o kh·ªõp v·ªõi truy v·∫•n.")
    #             break

    #         logger.info(f"  ƒê√£ l·∫•y ƒë∆∞·ª£c {len(hits)} b·∫£n ghi. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω tr√≠ch xu·∫•t t·ª´ kh√≥a...")

    #         for hit in tqdm(hits, desc="  ƒêang x·ª≠ l√Ω batch", leave=False):
    #             doc = hit['_source']
    #             keywords = get_keywords_for_document(doc.get('title', ''), doc.get('content', ''))
    #             keywords = [kw.lower() for kw in keywords]
    #             # keywords = [kw.lower() for kw in get_keywords_for_document(doc.get('title', ''), doc.get('content', ''))]

    #             new_doc = {}
    #             for field in fields_to_extract:
    #                 if field in doc:
    #                     new_doc[field] = doc[field]
                
    #             new_doc['key_word_extract'] = keywords
    #             records_to_bulk.append(new_doc)
    #             total_processed += 1

    #         if len(records_to_bulk) >= bulk_batch_size:
    #             logger.info(f"  ƒêang ƒë·∫©y {len(records_to_bulk)} b·∫£n ghi l√™n '{target_index_alias}'...")
    #             bulk_data_to_elasticsearch_kw_a(es, records_to_bulk, target_index_alias)
    #             records_to_bulk = []

    #         search_after_value = hits[-1]['sort']
        
    #     if records_to_bulk:
    #         logger.info(f"  ƒê·∫©y {len(records_to_bulk)} b·∫£n ghi cu·ªëi c√πng l√™n '{target_index_alias}'...")
    #         bulk_data_to_elasticsearch_kw_a(es, records_to_bulk, target_index_alias)

    #     logger.info(f"üéâ HO√ÄN T·∫§T! ƒê√£ x·ª≠ l√Ω v√† ƒë·∫©y t·ªïng c·ªông {total_processed} b·∫£n ghi.")

    # except Exception as e:
    #     logger.error(f"‚ùå L·ªói nghi√™m tr·ªçng trong v√≤ng l·∫∑p x·ª≠ l√Ω: {e}", exc_info=True)
    
    return
