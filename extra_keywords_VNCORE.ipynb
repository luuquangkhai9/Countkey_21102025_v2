{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7801cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh v·ªÅ th∆∞ m·ª•c c·ª• th·ªÉ (v√≠ d·ª•: C:\\vncorenlp)\n",
    "py_vncorenlp.download_model(save_dir=r\"C:\\Users\\Administrator\\Documents\\KeyWord_Trending\\vncorenlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e935c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "model = py_vncorenlp.VnCoreNLP(save_dir=r'/home/chiennd/Desktop/khailq/vncorenlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabcfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = \"M·ªπ √°p d·ª•ng thu·∫ø quan v·ªõi c√°c n∆∞·ªõc ch√¢u √° nh∆∞ Myanmar, Trung Qu·ªëc, Nh·∫≠t B·∫£n\"\n",
    "annotated_test = model.annotate_text(text_test)\n",
    "model.print_out(annotated_test)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f06d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "# Th√™m th∆∞ vi·ªán ƒë·ªÉ chia chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- C·∫•u h√¨nh ---\n",
    "INPUT_FILE_PATH = r'/home/chiennd/Desktop/khailq/data/test3.ndjson'\n",
    "OUTPUT_FILE_PATH = r'/home/chiennd/Desktop/khailq/result/olddata_keywords_in2609_chunked.ndjson'\n",
    "\n",
    "PRINT_KEYWORDS_PER_ARTICLE = True # ƒê·∫∑t l√† True ƒë·ªÉ in keywords c·ªßa t·ª´ng b√†i\n",
    "# THAY ƒê·ªîI: Th√™m c·∫•u h√¨nh ƒë·ªÉ log c√°c b√†i b√°o qu√° l·ªõn\n",
    "LOG_OVERSIZED_ARTICLES = True # ƒê·∫∑t l√† True ƒë·ªÉ in c·∫£nh b√°o khi b√†i b√°o c√≥ > 3 chunks\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'[-‚Äì‚Äî]+', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    stopwords = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word = line.strip().strip('\\\"')\n",
    "            if word:\n",
    "                stopwords.append(word.lower())\n",
    "    return tuple(stopwords)\n",
    "\n",
    "BLACKLISTED_START_WORDS = load_stopwords(r\"/home/chiennd/Desktop/khailq/blacklist/blacklist_keywords.txt\")\n",
    "\n",
    "CHUNK_GRAMMAR = [\n",
    "    (\"LEGAL_DOC_RULE_2\", r\"(?:<Np>|<N>)(?:\\s+<N>)*(?:\\s+<M>)+(?:\\s+(?:<Np>|<N>|<Ny>))+\"),\n",
    "    (\"LEGAL_DOC_RULE\", r\"(?:<Np>|<N>)(?:\\s+<N>)*(\\s+<M>)+\"),\n",
    "    (\"NOUN_RULE\", r\"(?:<Np>|<N>|<Ny>)(?:\\s+(?:<Np>|<N>|<Ny>))*\"),\n",
    "]\n",
    "\n",
    "def compile_rules(rules):\n",
    "    return [\n",
    "        (name, re.compile(rule.replace(\"<\", r\"(?:\\b\").replace(\">\", r\"\\b)\")))\n",
    "        for name, rule in rules\n",
    "    ]\n",
    "\n",
    "COMPILED_CHUNK_RULES = compile_rules(CHUNK_GRAMMAR)\n",
    "\n",
    "def is_valid_single_word_keyword(chunk_word, rule_name):\n",
    "    tag = chunk_word.get('posTag')\n",
    "    word_form = chunk_word.get('wordForm', '')\n",
    "    if tag == 'Np' and len(word_form) > 5:\n",
    "        return True\n",
    "    if tag == 'Ny':\n",
    "        if word_form.isupper() and len(word_form) >= 2:\n",
    "            return True\n",
    "    if rule_name == \"VERB_RULE\" and tag == 'V' and len(word_form) > 5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_keywords_2(annotated_data):\n",
    "    seen_keywords_lower = set()\n",
    "    ordered_keywords = []\n",
    "    if not isinstance(annotated_data, dict):\n",
    "        return []\n",
    "    for sentence_words in annotated_data.values():\n",
    "        if not sentence_words:\n",
    "            continue\n",
    "        pos_sequence = \" \".join([word.get('posTag', '') for word in sentence_words])\n",
    "        claimed_indices = [False] * len(sentence_words)\n",
    "        for rule_name, rule_pattern in COMPILED_CHUNK_RULES:\n",
    "            for match in rule_pattern.finditer(pos_sequence):\n",
    "                start_char, end_char = match.span()\n",
    "                start_word = len(pos_sequence[:start_char].strip().split()) if start_char > 0 else 0\n",
    "                end_word = len(pos_sequence[:end_char].strip().split())\n",
    "                if any(claimed_indices[i] for i in range(start_word, end_word)):\n",
    "                    continue\n",
    "                chunk_words = sentence_words[start_word:end_word]\n",
    "                phrase_text = ' '.join(w.get('wordForm', '') for w in chunk_words).replace(\"_\", \" \")\n",
    "                is_valid = False\n",
    "                if len(chunk_words) > 1:\n",
    "                    is_valid = True\n",
    "                elif len(chunk_words) == 1:\n",
    "                    if is_valid_single_word_keyword(chunk_words[0], rule_name):\n",
    "                        is_valid = True\n",
    "                if is_valid and phrase_text:\n",
    "                    lower_phrase = phrase_text.lower()\n",
    "                    if lower_phrase not in seen_keywords_lower:\n",
    "                        is_blacklisted = False\n",
    "                        for blacklisted_word in BLACKLISTED_START_WORDS:\n",
    "                            if blacklisted_word in lower_phrase:\n",
    "                                is_blacklisted = True\n",
    "                                break\n",
    "                        if is_blacklisted:\n",
    "                            continue\n",
    "                        seen_keywords_lower.add(lower_phrase)\n",
    "                        ordered_keywords.append(phrase_text)\n",
    "                        for i in range(start_word, end_word):\n",
    "                            claimed_indices[i] = True\n",
    "    return ordered_keywords\n",
    "\n",
    "\n",
    "# --- H√†m x·ª≠ l√Ω ch√≠nh \n",
    "def process_articles():\n",
    "    \"\"\"\n",
    "    ƒê·ªçc c√°c b√†i b√°o t·ª´ file NDJSON, chia nh·ªè vƒÉn b·∫£n, tr√≠ch xu·∫•t keywords v√† l∆∞u v√†o file m·ªõi.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50)\n",
    "\n",
    "    try:\n",
    "        with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as infile, \\\n",
    "             open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "            lines = infile.readlines()\n",
    "            print(f\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(lines)} b√†i b√°o t·ª´ file '{INPUT_FILE_PATH}'...\")\n",
    "\n",
    "            for line in tqdm(lines, desc=\"ƒêang tr√≠ch xu·∫•t keywords\"):\n",
    "                try:\n",
    "                    article_data = json.loads(line)\n",
    "                    content = article_data.get('content', '')\n",
    "                    title = article_data.get('title', '')\n",
    "                    combined_text = f\"{title}. {content}\" if title else content\n",
    "                    \n",
    "                    keywords = []\n",
    "                    if combined_text.strip() and combined_text.strip() != '.':\n",
    "                        chunks = text_splitter.split_text(combined_text)\n",
    "                        \n",
    "\n",
    "                        original_chunk_count = len(chunks)\n",
    "                        if original_chunk_count > 3:\n",
    "                            if LOG_OVERSIZED_ARTICLES:\n",
    "                                article_id = article_data.get('_id', 'Unknown ID')\n",
    "\n",
    "                                tqdm.write(f\"  Article ID: {article_id} has {original_chunk_count} chunks. Processing first chunk only.\")\n",
    "                            chunks = chunks[:1] # Ch·ªâ x·ª≠ l√Ω chunk ƒë·∫ßu ti√™n\n",
    "                        \n",
    "                        final_keywords_for_article = []\n",
    "                        seen_keywords_for_article = set()\n",
    "                        \n",
    "                        for chunk in chunks:\n",
    "                            cleaned_chunk = clean_text(chunk)\n",
    "                            if not cleaned_chunk:\n",
    "                                continue\n",
    "                            \n",
    "                            annotated_chunk = model.annotate_text(cleaned_chunk)\n",
    "                            keywords_from_chunk = extract_keywords_2(annotated_chunk)\n",
    "                            \n",
    "                            for keyword in keywords_from_chunk:\n",
    "                                lower_keyword = keyword.lower()\n",
    "                                if lower_keyword not in seen_keywords_for_article:\n",
    "                                    seen_keywords_for_article.add(lower_keyword)\n",
    "                                    final_keywords_for_article.append(keyword)\n",
    "                        \n",
    "                        keywords = final_keywords_for_article\n",
    "\n",
    "                    article_data['keywords'] = keywords\n",
    "                    outfile.write(json.dumps(article_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                    if PRINT_KEYWORDS_PER_ARTICLE:\n",
    "                        article_id = article_data.get('_id', 'Unknown ID')\n",
    "                        tqdm.write(f\"‚úÖ ID: {article_id} - Keywords ({len(keywords)}): {keywords}\")\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    tqdm.write(f\"‚ö†Ô∏è B·ªè qua d√≤ng kh√¥ng h·ª£p l·ªá: {line.strip()}\")\n",
    "                except Exception as e:\n",
    "                    tqdm.write(f\"‚ùå ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω d√≤ng: {line.strip()} - L·ªói: {e}\")\n",
    "\n",
    "            print(f\"\\\\nüéâ X·ª≠ l√Ω ho√†n t·∫•t! K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o file '{OUTPUT_FILE_PATH}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file ƒë·∫ßu v√†o '{INPUT_FILE_PATH}'. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ƒê√£ c√≥ l·ªói kh√¥ng mong mu·ªën x·∫£y ra: {e}\")\n",
    "\n",
    "# --- ƒêi·ªÉm b·∫Øt ƒë·∫ßu th·ª±c thi ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Kh·ªüi ƒë·ªông quy tr√¨nh x·ª≠ l√Ω b√†i b√°o v·ªõi VnCoreNLP...\")\n",
    "    process_articles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
